{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47630596",
   "metadata": {},
   "source": [
    "# 🔍 MVTec Anomaly Detection - Production Training\n",
    "\n",
    "**Multi-Category Autoencoder Training for Industrial Anomaly Detection**\n",
    "\n",
    "This notebook implements a production-ready training pipeline for anomaly detection using a UNet-style autoencoder on the MVTec AD dataset. The model learns to reconstruct normal images from all categories (screw, capsule, hazelnut) and detects anomalies based on reconstruction error.\n",
    "\n",
    "## Key Features:\n",
    "- ✅ **Multi-category training** on all \"good\" images\n",
    "- ✅ **Production-ready architecture** with proper error handling  \n",
    "- ✅ **Comprehensive evaluation** with metrics and visualizations\n",
    "- ✅ **Optimized threshold selection** for anomaly detection\n",
    "- ✅ **Model persistence** for deployment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23552517",
   "metadata": {},
   "source": [
    "## 1️⃣ Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b2db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install torch torchvision matplotlib seaborn scikit-learn pandas pillow numpy -q\n",
    "\n",
    "print(\"✅ Package installation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Add project src to Python path\n",
    "sys.path.insert(0, os.path.abspath(\"../src\"))\n",
    "sys.path.insert(0, os.path.abspath(\"../\"))\n",
    "\n",
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Data science and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score, \n",
    "    recall_score, f1_score, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check environment\n",
    "print(\"🔧 Environment Information:\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   Working directory: {os.getcwd()}\")\n",
    "print(f\"   Python version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"   Using device: {device}\")\n",
    "\n",
    "print(\"\\n✅ Environment setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6856d",
   "metadata": {},
   "source": [
    "## 2️⃣ Dataset Loading and Preprocessing\n",
    "\n",
    "We load all \"good\" images from all categories for training and prepare test sets for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7450c23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATA_ROOT = \"../data/mvtec_ad\"\n",
    "CATEGORIES = [\"screw\", \"capsule\", \"hazelnut\"]\n",
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Image transformations for training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "class MVTecDataset(Dataset):\n",
    "    \"\"\"Custom dataset for MVTec AD images.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, image_path\n",
    "\n",
    "print(\"✅ Dataset class and transforms defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e1f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training images (all \"good\" images from all categories)\n",
    "print(\"📂 Loading training images...\")\n",
    "\n",
    "train_images = []\n",
    "category_counts = {}\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    train_path = f\"{DATA_ROOT}/{category}/train/good\"\n",
    "    if os.path.exists(train_path):\n",
    "        category_images = glob.glob(f\"{train_path}/*.png\")\n",
    "        train_images.extend(category_images)\n",
    "        category_counts[category] = len(category_images)\n",
    "        print(f\"   {category}: {len(category_images)} images\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ Warning: {train_path} not found\")\n",
    "\n",
    "print(f\"\\n📊 Total training images: {len(train_images)}\")\n",
    "print(f\"📊 Category distribution: {category_counts}\")\n",
    "\n",
    "# Create training dataset and dataloader\n",
    "train_dataset = MVTecDataset(train_images, transform=transform)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"✅ Training DataLoader created with batch size {BATCH_SIZE}\")\n",
    "print(f\"   Total batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7579a23",
   "metadata": {},
   "source": [
    "### 📊 Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's analyze the training images to check for data quality issues and ensure consistency across categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa4e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Analyze image dimensions and file sizes\n",
    "print(\"🔍 Analyzing image properties...\")\n",
    "\n",
    "image_info = []\n",
    "corrupted_files = []\n",
    "\n",
    "# Sample analysis (check first 50 images to avoid long processing)\n",
    "sample_images = train_images[:min(50, len(train_images))]\n",
    "\n",
    "for img_path in sample_images:\n",
    "    try:\n",
    "        with Image.open(img_path) as img:\n",
    "            # Extract category from path\n",
    "            category = img_path.split('/')[-4]\n",
    "            \n",
    "            image_info.append({\n",
    "                'path': img_path,\n",
    "                'category': category,\n",
    "                'width': img.width,\n",
    "                'height': img.height,\n",
    "                'mode': img.mode,\n",
    "                'size_kb': os.path.getsize(img_path) / 1024\n",
    "            })\n",
    "    except Exception as e:\n",
    "        corrupted_files.append({'path': img_path, 'error': str(e)})\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df_images = pd.DataFrame(image_info)\n",
    "\n",
    "print(f\"📊 Sample Analysis Results ({len(sample_images)} images):\")\n",
    "print(f\"   Corrupted files: {len(corrupted_files)}\")\n",
    "\n",
    "if len(corrupted_files) > 0:\n",
    "    print(\"⚠️ Corrupted files found:\")\n",
    "    for cf in corrupted_files:\n",
    "        print(f\"   - {cf['path']}: {cf['error']}\")\n",
    "\n",
    "# Display basic statistics\n",
    "if len(df_images) > 0:\n",
    "    print(f\"\\n📏 Image Dimensions:\")\n",
    "    print(f\"   Width range: {df_images['width'].min()}-{df_images['width'].max()}\")\n",
    "    print(f\"   Height range: {df_images['height'].min()}-{df_images['height'].max()}\")\n",
    "    print(f\"   Most common size: {df_images.groupby(['width', 'height']).size().idxmax()}\")\n",
    "    \n",
    "    print(f\"\\n🎨 Image Modes:\")\n",
    "    print(df_images['mode'].value_counts())\n",
    "    \n",
    "    print(f\"\\n💾 File Sizes:\")\n",
    "    print(f\"   Average: {df_images['size_kb'].mean():.1f} KB\")\n",
    "    print(f\"   Range: {df_images['size_kb'].min():.1f}-{df_images['size_kb'].max():.1f} KB\")\n",
    "    \n",
    "    print(f\"\\n📂 Category Distribution (sample):\")\n",
    "    print(df_images['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0558f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Visual inspection of sample images\n",
    "print(\"\\n🖼️ Visual Sample Inspection:\")\n",
    "\n",
    "# Select 2 random images per category for visual check\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 12))\n",
    "fig.suptitle(\"Sample Training Images by Category\", fontsize=16)\n",
    "\n",
    "for i, category in enumerate(CATEGORIES):\n",
    "    category_images = [img for img in train_images if f'/{category}/' in img]\n",
    "    \n",
    "    if len(category_images) >= 2:\n",
    "        # Select 2 random samples\n",
    "        samples = np.random.choice(category_images, 2, replace=False)\n",
    "        \n",
    "        for j, img_path in enumerate(samples):\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                axes[i, j].imshow(img)\n",
    "                axes[i, j].set_title(f\"{category} - {img.size}\")\n",
    "                axes[i, j].axis('off')\n",
    "            except Exception as e:\n",
    "                axes[i, j].text(0.5, 0.5, f\"Error loading\\n{e}\", \n",
    "                               ha='center', va='center', transform=axes[i, j].transAxes)\n",
    "                axes[i, j].set_title(f\"{category} - ERROR\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Visual inspection completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa79a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pixel intensity analysis\n",
    "print(\"\\n📈 Pixel Intensity Analysis:\")\n",
    "\n",
    "# Sample a few images for pixel analysis\n",
    "sample_size = min(10, len(train_images))\n",
    "pixel_stats = []\n",
    "\n",
    "for img_path in np.random.choice(train_images, sample_size, replace=False):\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Calculate statistics per channel\n",
    "        for channel, name in enumerate(['R', 'G', 'B']):\n",
    "            channel_data = img_array[:, :, channel]\n",
    "            pixel_stats.append({\n",
    "                'category': img_path.split('/')[-4],\n",
    "                'channel': name,\n",
    "                'mean': channel_data.mean(),\n",
    "                'std': channel_data.std(),\n",
    "                'min': channel_data.min(),\n",
    "                'max': channel_data.max()\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {img_path}: {e}\")\n",
    "\n",
    "if pixel_stats:\n",
    "    df_pixels = pd.DataFrame(pixel_stats)\n",
    "    \n",
    "    print(\"📊 Pixel Intensity Statistics (0-255 range):\")\n",
    "    print(f\"   Mean values: {df_pixels['mean'].mean():.1f} ± {df_pixels['mean'].std():.1f}\")\n",
    "    print(f\"   Min values: {df_pixels['min'].min():.0f}\")\n",
    "    print(f\"   Max values: {df_pixels['max'].max():.0f}\")\n",
    "    \n",
    "    # Check for any unusual ranges\n",
    "    if df_pixels['min'].min() < 0:\n",
    "        print(\"⚠️ Warning: Found negative pixel values!\")\n",
    "    if df_pixels['max'].max() > 255:\n",
    "        print(\"⚠️ Warning: Found pixel values > 255!\")\n",
    "    \n",
    "    print(\"\\n📈 Statistics by Category:\")\n",
    "    category_stats = df_pixels.groupby('category')['mean'].agg(['mean', 'std']).round(1)\n",
    "    print(category_stats)\n",
    "\n",
    "print(\"\\n✅ Pixel analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ETL Requirements Assessment\n",
    "print(\"\\n🔍 ETL ASSESSMENT:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if additional ETL is needed\n",
    "etl_needed = []\n",
    "etl_recommendations = []\n",
    "\n",
    "# Check 1: Consistent image dimensions\n",
    "if len(df_images) > 0:\n",
    "    unique_sizes = df_images.groupby(['width', 'height']).size()\n",
    "    if len(unique_sizes) > 1:\n",
    "        etl_needed.append(\"Image resizing\")\n",
    "        etl_recommendations.append(\"✅ HANDLED: Images resized to 128x128 in transforms\")\n",
    "    else:\n",
    "        etl_recommendations.append(\"✅ OK: All images have consistent dimensions\")\n",
    "\n",
    "# Check 2: Color mode consistency\n",
    "if len(df_images) > 0 and len(df_images['mode'].unique()) > 1:\n",
    "    etl_needed.append(\"Color mode standardization\")\n",
    "    etl_recommendations.append(\"✅ HANDLED: Images converted to RGB in dataset\")\n",
    "else:\n",
    "    etl_recommendations.append(\"✅ OK: Consistent color modes\")\n",
    "\n",
    "# Check 3: Corrupted files\n",
    "if len(corrupted_files) > 0:\n",
    "    etl_needed.append(\"Remove corrupted files\")\n",
    "    etl_recommendations.append(f\"⚠️ ACTION NEEDED: Remove {len(corrupted_files)} corrupted files\")\n",
    "else:\n",
    "    etl_recommendations.append(\"✅ OK: No corrupted files detected\")\n",
    "\n",
    "# Check 4: Pixel value normalization\n",
    "etl_recommendations.append(\"✅ HANDLED: Pixel normalization to [-1,1] in transforms\")\n",
    "\n",
    "# Summary\n",
    "print(\"📋 ETL STATUS:\")\n",
    "if len(etl_needed) == 0 or all(\"HANDLED\" in rec for rec in etl_recommendations):\n",
    "    print(\"🎉 READY FOR TRAINING!\")\n",
    "    print(\"   All necessary ETL operations are handled by the pipeline\")\n",
    "else:\n",
    "    print(\"⚠️ ADDITIONAL ETL NEEDED:\")\n",
    "    for need in etl_needed:\n",
    "        print(f\"   - {need}\")\n",
    "\n",
    "print(\"\\n📝 ETL PIPELINE SUMMARY:\")\n",
    "for rec in etl_recommendations:\n",
    "    print(f\"   {rec}\")\n",
    "\n",
    "print(\"\\n🔧 CURRENT ETL OPERATIONS:\")\n",
    "print(\"   1. ✅ Image loading and RGB conversion\")\n",
    "print(\"   2. ✅ Resize to 128x128 pixels\")  \n",
    "print(\"   3. ✅ Tensor conversion\")\n",
    "print(\"   4. ✅ Normalization to [-1, 1] range\")\n",
    "print(\"   5. ✅ Batch loading with DataLoader\")\n",
    "\n",
    "print(\"\\n💡 CONCLUSION:\")\n",
    "print(\"   The current ETL pipeline is sufficient for training!\")\n",
    "print(\"   No additional preprocessing steps required.\")\n",
    "\n",
    "print(\"\\n✅ EDA completed - Ready to proceed with training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580fd1e9",
   "metadata": {},
   "source": [
    "## 3️⃣ Model Architecture Definition\n",
    "\n",
    "We use a UNet-style autoencoder with skip connections for better feature preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dad61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderUNetLite(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet-style Autoencoder for anomaly detection.\n",
    "    \n",
    "    Architecture:\n",
    "    - Encoder: 3 levels with max pooling\n",
    "    - Bottleneck: Feature compression  \n",
    "    - Decoder: 3 levels with transposed convolutions\n",
    "    - Skip connections: Preserve fine-grained features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(AutoencoderUNetLite, self).__init__()\n",
    "        \n",
    "        # Encoder path\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Decoder path\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),  # 512 due to skip connection\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),  # 256 due to skip connection\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),   # 128 due to skip connection\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Final output layer\n",
    "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        self.tanh = nn.Tanh()  # Output in [-1, 1] range\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder path with skip connection storage\n",
    "        enc1 = self.enc1(x)          # 128x128x64\n",
    "        pool1 = self.pool1(enc1)     # 64x64x64\n",
    "        \n",
    "        enc2 = self.enc2(pool1)      # 64x64x128\n",
    "        pool2 = self.pool2(enc2)     # 32x32x128\n",
    "        \n",
    "        enc3 = self.enc3(pool2)      # 32x32x256\n",
    "        pool3 = self.pool3(enc3)     # 16x16x256\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(pool3)  # 16x16x512\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        up3 = self.upconv3(bottleneck)       # 32x32x256\n",
    "        merge3 = torch.cat([up3, enc3], dim=1)  # 32x32x512\n",
    "        dec3 = self.dec3(merge3)             # 32x32x256\n",
    "        \n",
    "        up2 = self.upconv2(dec3)             # 64x64x128\n",
    "        merge2 = torch.cat([up2, enc2], dim=1)  # 64x64x256\n",
    "        dec2 = self.dec2(merge2)             # 64x64x128\n",
    "        \n",
    "        up1 = self.upconv1(dec2)             # 128x128x64\n",
    "        merge1 = torch.cat([up1, enc1], dim=1)  # 128x128x128\n",
    "        dec1 = self.dec1(merge1)             # 128x128x64\n",
    "        \n",
    "        # Final output\n",
    "        output = self.final(dec1)            # 128x128x3\n",
    "        return self.tanh(output)\n",
    "\n",
    "# Create model instance\n",
    "model = AutoencoderUNetLite().to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"🏗️ Model Architecture:\")\n",
    "print(f\"   Model: AutoencoderUNetLite\")\n",
    "print(f\"   Input shape: {IMAGE_SIZE}x{IMAGE_SIZE}x3\")\n",
    "print(f\"   Output shape: {IMAGE_SIZE}x{IMAGE_SIZE}x3\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "print(\"\\n✅ Model architecture defined and initialized!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
